<script src="https://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<html>
  <link href="style.css" type="text/css" rel="stylesheet" />
  <head>
    <title>Visual Memory for Robust Path Following</title>
  </head>
  <meta content="Maps and Landmarks" property="og:title" />
</html>
<body>
  <center>
    <table width="1000px">
      <tr>
        <td>
          <center>
            <span style="font-size:42px">Visual Memory for Robust Path Following</span>
          </center>
          <br />
        </td>
      </tr>
      <tr align="center">
        <td>
          <table width="80%">
            <tr>
              <td width="19%">
                <center>
                  <span style="font-size:20px">
                    <a href="https://ashishkumar1993.github.io/">Ashish Kumar*</a>
                  </span>
                </center>
              </td>

              <td width="19%">
                <center>
                  <span style="font-size:20px">
                    <a href="https://people.eecs.berkeley.edu/~sgupta">Saurabh Gupta*</a>
                  </span>
                </center>
              </td>
              <td width="19%">
                <center>
                  <span style="font-size:20px">
                    <a href="https://people.eecs.berkeley.edu/~dfouhey">David Fouhey</a>
                  </span>
                </center>
              </td>
              <td width="19%">
                <center>
                  <span style="font-size:20px">
                    <a href="https://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>
                  </span>
                </center>
              </td>
              <td width="19%">
                <center>
                  <span style="font-size:20px">
                    <a href="https://people.eecs.berkeley.edu/~malik">Jitendra Malik</a>
                  </span>
                </center>
              </td>
            </tr>
          </table>
          <br />
        </td>
      </tr>
      <tr>
        <td>
          <center>
            <span style="font-size:20px">University of California at Berkeley</span>
          </center>
          <br />
        </td>
      </tr>
      <tr>
        <td>
          <table>
            <tr>
              <td>
                <center>
                <img src="./resources/images/teaser.jpg" width="80%" />
                </center>
              </td>
            </tr>
            <tr>
              <td>
                <center>
                  <span style="font-size:14px">
                  <i><span style='font-weight:bold'>Problem Setup:</span> Given
                  a set of reference images and poses, the starting image and
                  pose, and a goal pose, we want a policy Ï€ that is able to
                  convey the robot from its current pose to the target pose
                  using first person RGB image observations under noisy
                  actuation.</i>
                  </span>
                </center>
                <br />
              </td>
            </tr>
            <tr>
              <td>
Humans routinely retrace paths in a novel environment both forwards and backwards despite uncertainty in their motion. This paper presents an approach for doing so. Given a demonstration of a path, a first network generates a path abstraction. Equipped with this abstraction, a second network observes the world and decides how to act to retrace the path under noisy actuation and a changing environment. The two networks are optimized end-to-end at training time. We evaluate the method in two realistic simulators, performing path following and homing under actuation noise and environmental changes. Our experiments show that our approach outperforms classical approaches and other learning based baselines.
<br /><br /><hr /></td>
            </tr>
          </table>
        </td>
      </tr>
      <tr>
        <td>
          <center>
            <h1>Paper</h1>
            <table width="100%">
              <tr>
                <td width="30%">
                  <a href="https://saurabhg.web.illinois.edu/pdfs/kumar2018visual.pdf">
                    <img src="./resources/images/teaser.jpg" width="100%" />
                  </a>
                </td>
                <td width="2%"></td>
                <td width="60%"><b>Visual Memory for Robust Path Following<br /></b>
                Ashish Kumar*, Saurabh Gupta*, David Fouhey, Sergey Levine, Jitendra Malik<br />
                To appear at NeurIPS 2018 <span style='color:#a00;font-weight:bold'>(Oral)</span><br/> <br/>
                <a href="./resources/bibtex.bib">bibtex</a> / 
                <a href="https://saurabhg.web.illinois.edu/pdfs/kumar2018visual.pdf">pdf</a></td>
              </tr>
            </table>
          </center>
          <br />
          <hr />
        </td>
      </tr>

      <tr><td>
      <center><h1>What's the problem?</h1></centeR>

      <img src='resources/images/fig1.png' width='100%'><br/><br/>
        <p>
        The main idea is that we are given a <b>single</b> demonstration of a path consisting of observations and
        actions. Our goal is to re-execute this path either forwards (i.e.,
        following it) or backwards (i.e., homing behavior). The difficulty is that
        as we execute this path, our actions are <b>noisy</b> and the world may <b>change</b>
        in the meantime. Both mean that blind replay of actions will not succeed --
        if we simply replay the actions, we may end up somewhere else (try going from your
        bed to your office while blindfolded) or we may end up bumping into things (the
        pedestrians you avoided yesterday are not in the same place today). 
        </p>
        <p>
        The paper presents a method, described below, that aims to solve this task and compares
        it with alternate approaches on two environments.
        </p>
      <hr/>
      </td></tr>
      <tr>
        <td>
        <center><h1>Results</h1></center>
        
        <p style="text-indent: 40px">
        We present a video visualization for the path following experiments. On the left
        is the overhead view (<b>which the agent does not have access to</b>). In the middle
        is the demonstration, which the agent has to follow (either forwards or backwards).
        On the right is the execution as done by the agent. 
        <br/><br/><br/>

        First, we show following a path forwards.
        <video width='100%' controls> <source src='http://web.eecs.umich.edu/~fouhey/TEMP/RPF//Media1.mp4'></video>
        <br/><br/><br/>

        Next, we show following a path backwards. Note the fact that there are two doors. To successfully retrace
        the path backwards, the agent must identify which door it came out of.
        <video width='100%' controls> <source src='http://web.eecs.umich.edu/~fouhey/TEMP/RPF//Media2.mp4'></video>
        <br/><br/><br/>

        Finally, we show executing a path with the world changing in between demonstration and execution (note that the
        bed is missing). Here we show both open loop as well as our proposed method. 

        <video width='100%' controls> <source src='http://web.eecs.umich.edu/~fouhey/TEMP/RPF//Media3.mp4'></video><br/>

        </p>

   
    <!--
    <table><tr><td width="49%"><iframe src="https://www.youtube.com/embed/UoX3DAkxaSA?ecver=1" height="300" width="490" allowfullscreen="1" gesture="media" allow="encrypted-media" frameborder="0"></iframe></td><td width="2%"></td><td width="49%"><iframe src="https://www.youtube.com/embed/W9nWp9HN3V8?ecver=1" height="300" width="490" allowfullscreen="1" gesture="media" allow="encrypted-media" frameborder="0"></iframe></td></tr></table><br />
    
    
    --> 
    <hr /></td>
      </tr>

      <tr><td>
        <center><h1>How does it work?</h1></center>
        <img src='resources/images/fig2.png' width='100%'><br/><br/>

        <p>
        The method contains two components: a network set of networks
        &phi; and &Psi; that abstract
        the image observations seen while the path is demonstrated into a series
        of vectors that are easily digested by a learning method;
        and a recurrent network &pi; that attends to this sequence
        of vectors, looks through the camera, and chooses an action. 
        </p>    

        <p>
        The job of &phi; and &Psi; is to convert the image and action observations 
        from the path demonstration into something a learning system can handle.
        It consists of a CNN, &phi; that is applied to each image, as well a two layer 
        fully-connected network &Psi; that blends together image and action observations.
        </p>

        <p>
        The job of &pi; is to use the path abstraction to re-execute the path
        under noisy actuation and a changing world, and is implemented as a
        GRU. As input, &pi; looks at the image, its previous state, and the path
        abstraction. Rather than look at all of the images at once, &pi; has a pointer
        &eta; into the abstraction that it uses to softly attend to the steps of the 
        execution. As it executes the path, &pi; updates where this pointer &eta; is
        pointing.  
        </p>

        <p>
        The entire system is end-to-end trainable from data. We train it using 
        imitation learning on 120K episodes. Each episode consists of a 30 step path which
        the agent is given 40 steps to execute (each step is ~40cm).

        </p>


      
      <hr/> 
      </td></br>



      <tr>
        <td>
          <center>
            <h1>Acknowledgments</h1>
            <table width="100%">
              <tr>
                <td width="100%">This work was supported in part by Intel/NSF VEC award IIS-1539099, and the Google Fellowship to SG.This webpage template was borrowed from some <a href="https://richzhang.github.io/colorization/">colorful folks.</a></td>
              </tr>
            </table>
          </center>
        </td>
      </tr>
    </table>
  </center>
</body>
